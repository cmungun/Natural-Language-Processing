{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Introduction to NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classwork 22/5/2020\n",
    "#nltk: python nltk package--> natural language tool kit-->for NLP tasks\n",
    "#open source\n",
    "#nltk-->task performs-->tokenization, stemming,tagging,classification\n",
    "#import nltk package\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Download NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling nltk downloader\n",
    "#nltk.download()\n",
    "#use to choose different corpus and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 2 - IMPORT BROWN CORPUS AND ACCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Thirty-three',\n",
       " 'Scotty',\n",
       " 'did',\n",
       " 'not',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'school',\n",
       " '.',\n",
       " 'His']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#brown corpus----> 0ne million words,published in the US in 1961\n",
    "#English text\n",
    "#set of corpora which can be read using the nltk.corpus package\n",
    "#corpus contains text from 500 sources\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#brown corpus contains various categories\n",
    "#categories()---->the categories of the corpus\n",
    "print(brown.categories())\n",
    "\n",
    "#brown.words(categories='adventure')[:10]\n",
    "#plaintext corpora support methods to read the corpus  a list of words.\n",
    "#under categories----> fiction category choosen, display first 10 words.\n",
    "brown.words(categories='fiction')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. IMPORT INAUGRAL CORPUS AND ACCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " '-',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " ':',\n",
       " 'In',\n",
       " 'compliance',\n",
       " 'with',\n",
       " 'a',\n",
       " 'custom',\n",
       " 'as',\n",
       " 'old']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#includes every president's inaugral address from 1789 to 2009\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "#inaugural corpora contains set of files\n",
    "#each file have id\n",
    "print(inaugural.fileids())\n",
    "\n",
    "#inaugural.words(fileids = '2017-Trump.txt')[:50]\n",
    "#plaintext corpora support methods to read the corpus  a list of words.\n",
    "#file id 1861-Lincoln.txt selected and printing first 50 words\n",
    "inaugural.words(fileids = '1861-Lincoln.txt')[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4: IMPORTING WEBTEXT CORPUS AND ACCESS DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['firefox.txt', 'grail.txt', 'overheard.txt', 'pirates.txt', 'singles.txt', 'wine.txt']\n",
      "firefox.txt Cookie Manager: \"Don't allow sites that set removed cookies to set future cookies\" should stay check\n",
      "grail.txt SCENE 1: [wind] [clop clop clop] \n",
      "KING ARTHUR: Whoa there!  [clop clop clop] \n",
      "SOLDIER #1: Halt!  Who\n",
      "overheard.txt White guy: So, do you have any plans for this evening?\n",
      "Asian girl: Yeah, being angry!\n",
      "White guy: Oh,\n",
      "pirates.txt PIRATES OF THE CARRIBEAN: DEAD MAN'S CHEST, by Ted Elliott & Terry Rossio\n",
      "[view looking straight dow\n",
      "singles.txt 25 SEXY MALE, seeks attrac older single lady, for discreet encounters.\n",
      "35YO Security Guard, seeking \n",
      "wine.txt Lovely delicate, fragrant Rhone wine. Polished leather and strawberries. Perhaps a bit dilute, but g\n"
     ]
    }
   ],
   "source": [
    "#set of corpora which can be read using the nltk.corpus package.\n",
    "#corpora contains webtext\n",
    "#web text includes content from a Firefox,movie scripts\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "#printing fileids under webtext corpora\n",
    "print(webtext.fileids())\n",
    "#printing  fileids\n",
    "for fileids in webtext.fileids():\n",
    "    #the raw content of the specified files from webtext\n",
    "    print(fileids, webtext.raw(fileids)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5: FREQUENCY DISTRIBUTION OF WORDS IN A TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'and': 4, 'Trump': 2, 'the': 2, 'Donald': 1, 'John': 1, '(born': 1, 'June': 1, '14,': 1, '1946)': 1, 'is': 1, ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TASK 5: FREQUENCY DISTRIBUTION OF WORDS IN A TEXT\n",
    "text1 = 'Donald John Trump (born June 14, 1946) is the 45th and current president of the United States. Before entering politics, he was a businessman and television personality. it The Trump Organization, and expanded its operations from Queens and Brooklyn into Manhattan.'\n",
    "#text is splitted into words and count the frequency of each word\n",
    "#split()-->function to split text\n",
    "fd = nltk.FreqDist(text1.split())\n",
    "#printing frequencu of word\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 6.CONDITIONAL FREQUENCY DISTRIBUTION OF WORDS IN A TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Donald': 1, 'United': 1, 'Before': 1, 'Queens': 1})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.probability--->support for conditional frequency distribution\n",
    "#repeatedly running an experiment under a variety of conditions\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "#Use the indexing operator to access the frequency distribution for a given condition.\n",
    "cfd = ConditionalFreqDist((len(word), word) for word in text1.split())\n",
    "#words with length of 6 character\n",
    "cfd[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------Homework------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 - IMPORT INAUGURAL CORPUS AND ACCESSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Fellow',\n",
       " '-',\n",
       " 'Citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " ':',\n",
       " 'In',\n",
       " 'compliance',\n",
       " 'with',\n",
       " 'a',\n",
       " 'custom',\n",
       " 'as',\n",
       " 'old',\n",
       " 'as',\n",
       " 'the',\n",
       " 'Government',\n",
       " 'itself',\n",
       " ',',\n",
       " 'I',\n",
       " 'appear',\n",
       " 'before',\n",
       " 'you',\n",
       " 'to',\n",
       " 'address',\n",
       " 'you',\n",
       " 'briefly',\n",
       " 'and',\n",
       " 'to',\n",
       " 'take',\n",
       " 'in',\n",
       " 'your',\n",
       " 'presence',\n",
       " 'the',\n",
       " 'oath',\n",
       " 'prescribed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Constitution',\n",
       " 'of',\n",
       " 'the',\n",
       " 'United',\n",
       " 'States',\n",
       " 'to',\n",
       " 'be',\n",
       " 'taken',\n",
       " 'by',\n",
       " 'the',\n",
       " 'President']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HomeWork 22/5/2020\n",
    "#includes every president's inaugral address from 1789 to 2009\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "#inaugural corpora contains set of files\n",
    "#each file have id\n",
    "#the categories of the corpus corresponding to these files\n",
    "print(inaugural.fileids())\n",
    "\n",
    "#inaugural.words(fileids = '2017-Trump.txt')[:50]\n",
    "#he words of the specified fileids from inaugural corpora\n",
    "inaugural.words(fileids = '1861-Lincoln.txt')[:50]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK - 2: READ CONTENT OF THE TEXT FILE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you. Thank you so much.\n",
      "\n",
      "Vice President Biden, Mr. Chief Justice, Members of the United States Congress, distinguished guests, and fellow citizens:\n",
      "\n",
      "Each time we gather to inaugurate a President we bear witness to the enduring strength of our Constitution. We affirm the promise of our democracy. We recall that what binds this Nation together is not the colors of our skin or the tenets of our faith or the origins of our names. What makes us exceptionalâwhat makes us Americanâis our allegiance to an idea articulated in a declaration made more than two centuries ago:\n",
      "\n",
      "We hold these truths to be self-evident, that all men are created equal; that they are endowed by their Creator with certain unalienable rights; that among these are life, liberty, and the pursuit of happiness.\n",
      "\n",
      "Today we continue a never-ending journey to bridge the meaning of those words with the realities of our time. For history tells us that while these truths may be self-evident, they've never been self-execut\n"
     ]
    }
   ],
   "source": [
    "#plaintext corpora support methods to read the corpus  a list of words.\n",
    "#file id 2013-Obama.txt selected and printing whole text\n",
    "#raw()--->to read content of the corpus\n",
    "text = inaugural.raw('2013-Obama.txt')\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 3: READ WORDS OF THE TEXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['you',\n",
       " '.',\n",
       " 'Thank',\n",
       " 'you',\n",
       " 'so',\n",
       " 'much',\n",
       " '.',\n",
       " 'Vice',\n",
       " 'President',\n",
       " 'Biden',\n",
       " ',',\n",
       " 'Mr',\n",
       " '.',\n",
       " 'Chief',\n",
       " 'Justice',\n",
       " ',',\n",
       " 'Members',\n",
       " 'of',\n",
       " 'the']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words of the 2013-Obama.txt fileids from 1 to 20\n",
    "inaugural.words(fileids = '2013-Obama.txt')[1:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 4: FREQUENCY DISTRIBUTION OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'and': 4, 'Trump': 2, 'the': 2, 'Donald': 1, 'John': 1, '(born': 1, 'June': 1, '14,': 1, '1946)': 1, 'is': 1, ...})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text is splitted into words and count the frequency of each word\n",
    "#split()-->function to split text\n",
    "fd = nltk.FreqDist(text1.split())\n",
    "#printing frequencu of word\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 5: Conditional Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 9 conditions>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.probability--->support for conditional frequency distribution\n",
    "#repeatedly running an experiment under a variety of conditions\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "#Use the indexing operator to access the frequency distribution for a given condition.\n",
    "text = \"Vice President Biden, Mr. Chief Justice, Members of the United States Congress, distinguished guests, and fellow citizens\"\n",
    "cfd = ConditionalFreqDist((len(word), word) for word in text.split())\n",
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 6: Conditional Frequency Distribution for 4,5 6- letter words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Vice': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print conditional frequency distribution with word length:4\n",
    "cfd[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Chief': 1})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print conditional frequency distribution with word length:5\n",
    "cfd[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'Biden,': 1, 'United': 1, 'States': 1, 'fellow': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print conditional frequency distribution with word length:5\n",
    "cfd[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7: Words in Ascending Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      ".\n",
      "President\n",
      "Thank\n",
      "Thank\n",
      "Vice\n",
      "much\n",
      "so\n",
      "you\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "#the words of the 2013-Obama.txt fileid and sorting in ascending order\n",
    "word = inaugural.words(fileids = '2013-Obama.txt')[0:10]\n",
    "#sort()---> function for sorting the words in ascending order\n",
    "word.sort()\n",
    "for i in word:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Words in Descending Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you\n",
      "you\n",
      "so\n",
      "much\n",
      "Vice\n",
      "Thank\n",
      "Thank\n",
      "President\n",
      ".\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "#reverse = True----------->in descending order\n",
    "word.sort(reverse = True)\n",
    "#for each word\n",
    "for i in word:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------Additional----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Various Predefined Text Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "##various predefined text access by following command\n",
    "from nltk.book import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Import Reuters Corpus and Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2000, Reuters Ltd made available a large collection of Reuters News stories for use in research and development of natural language processing, information retrieval, and machine learning systems. This corpus, known as \"Reuters Corpus, Volume 1\" or RCV1, is significantly larger than the older, well-known Reuters-21578 collection heavily used in the text classification community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test/14826',\n",
       " 'test/14828',\n",
       " 'test/14829',\n",
       " 'test/14832',\n",
       " 'test/14833',\n",
       " 'test/14839',\n",
       " 'test/14840',\n",
       " 'test/14841',\n",
       " 'test/14842',\n",
       " 'test/14843']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import reuters\n",
    "#reuters corpora contains set of files\n",
    "#each file have id\n",
    "reuters.fileids()[:10]#'test/14826'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Read the Content of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\\n  Mounting trade friction between the\\n  U.S. And Japan has raised fears among many of Asia's exporting\\n  nations that the row could inflict far-reaching economic\\n  damage, businessmen and officials said.\\n      They told Reuter correspondents in Asian capitals a U.S.\\n  Move against Japan might boost protectionist sentiment in the\\n  U.S. And lead to curbs on American imports of their products.\\n      But some exporters said that while the conflict would hurt\\n  them in the long-run, in the short-term Tokyo's loss might be\\n  their gain.\\n      The U.S. Has said it will impose 300 mln dlrs of tariffs on\\n  imports of Japanese electronics goods on April 17, in\\n  retaliation for Japan's alleged failure to stick to a pact not\\n  to sell semiconductors on world markets at below cost.\\n      Unofficial Japanese estimates put the impact of the tariffs\\n  at 10 billion dlrs and spokesmen for major electronics firms\\n  said they would virtually halt exports\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print-->raw() content of a file\n",
    "reuters.raw('test/14826')[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Words That Occur Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['billion dlrs',\n",
       " 'Hong Kong',\n",
       " 'South Korea',\n",
       " 'Last year',\n",
       " 'Japan might',\n",
       " 'Button said',\n",
       " 'Japanese electronics',\n",
       " 'trade surplus',\n",
       " 'businessmen said']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import Text class from nltk.text\n",
    "from nltk.text import Text\n",
    "text_data = Text(reuters.words(fileids = 'test/14826'))\n",
    "#Collocation--------> Words that occur together\n",
    "text_data.collocation_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Find A Specific Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 4 of 4 matches:\n",
      " JAPAN RIFT Mounting trade friction between the U . S . And Japan has raised fe\n",
      " association said the trade dispute between the U . S . And Japan might also le\n",
      "awaiting the outcome of trade talks between the U . S . And Japan with interest\n",
      "of deterioration in trade relations between two countries which are major tradi\n"
     ]
    }
   ],
   "source": [
    "#concordance----------->To find a specific word in the text\n",
    "text_data.concordance(\"between\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Words in Specific Fileid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ASIAN',\n",
       " 'EXPORTERS',\n",
       " 'FEAR',\n",
       " 'DAMAGE',\n",
       " 'FROM',\n",
       " 'U',\n",
       " '.',\n",
       " 'S',\n",
       " '.-',\n",
       " 'JAPAN',\n",
       " 'RIFT',\n",
       " 'Mounting',\n",
       " 'trade',\n",
       " 'friction',\n",
       " 'between']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#words in specific file\n",
    "reuters.words(fileids = 'test/14826')[:15]#'test/14826'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 7 : Total Sentence, words in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences:  31\n",
      "Total Words:  899\n",
      "Total words in lower case:  375\n"
     ]
    }
   ],
   "source": [
    "#plaintext corpora support methods to read the corpus  a list of words.\n",
    "reuters1 = reuters.words(fileids = 'test/14826')\n",
    "#total words\n",
    "#len(president_word)\n",
    "\n",
    "#length of sentences\n",
    "print(\"Total sentences: \" ,len(reuters.sents('test/14826')))\n",
    "\n",
    "#length of words\n",
    "print(\"Total Words: \",len( reuters.words('test/14826')))\n",
    "\n",
    "#length of lower words\n",
    "print(\"Total words in lower case: \", len(set(w.lower() for w in reuters.words('test/14826'))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 8: Frequency of Words Matches With List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and: 16 you: 0 between: 4 said: 16 should: 0 they: 3 "
     ]
    }
   ],
   "source": [
    "#Frequency of words matches with list\n",
    "fdist = nltk.FreqDist(w.lower() for w in reuters1)\n",
    "\n",
    "#list of words\n",
    "list_words = ['and', 'you', 'between', 'said', 'should', 'they']\n",
    "\n",
    "#print all words with frequencies in text\n",
    "for i in list_words:\n",
    "     print(i + ':', fdist[i], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 9 Sentence startswith() Specified Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({False: 895, True: 4})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print frequency of sentences that start with 'this' using startswith() function\n",
    "fdist = nltk.FreqDist(w.startswith('between') for w in reuters1)\n",
    "\n",
    "#print frequency\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 10 Sort the Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['But',\n",
       " 'Button',\n",
       " 'Canberra',\n",
       " 'Capel',\n",
       " 'Co',\n",
       " 'DAMAGE',\n",
       " 'Democratic',\n",
       " 'Deputy',\n",
       " 'EXPORTERS',\n",
       " 'Electric',\n",
       " 'Exports',\n",
       " 'FEAR',\n",
       " 'FROM']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(set())---> function for sorting the words in ascending order\n",
    "sorted(set(text_data))[37:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 11: Reverse Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'in',\n",
       " 'week',\n",
       " 'this',\n",
       " 'Washington',\n",
       " 'in',\n",
       " 'meet',\n",
       " 'to',\n",
       " 'due',\n",
       " 'are',\n",
       " '),',\n",
       " 'MITI',\n",
       " '(',\n",
       " 'Industry',\n",
       " 'and',\n",
       " 'Trade',\n",
       " 'International',\n",
       " 'of',\n",
       " 'minister']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reverse words in descending order\n",
    "words = list(reversed(reuters1))\n",
    "words[6:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 12: Frequency of Each Word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 62, 'the': 32, 'of': 30, 'to': 26, ',': 20, 'U': 19, 'S': 19, 'said': 16, 'a': 14, 'trade': 13, ...})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the frequency of tokens\n",
    "#importing FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(reuters1)\n",
    "#prnt frequency\n",
    "fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 13: Length of Longest Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of longest sequences from text using max function\n",
    "longest_sentence = max(len(s) for s in reuters1)\n",
    "longest_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 14: Length of Smallest Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#length of smallest sentence using mix function\n",
    "smallest_sentence = min(len(s) for s in reuters1)\n",
    "smallest_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 15: Part of speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ASIAN', 'NNP'), ('EXPORTERS', 'NNP'), ('FEAR', 'NNP'), ('DAMAGE', 'NNP'), ('FROM', 'NNP'), ('U', 'NNP'), ('.', '.'), ('S', 'NNP'), ('.-', 'JJ'), ('JAPAN', 'NNP'), ('RIFT', 'NNP'), ('Mounting', 'NNP'), ('trade', 'NN'), ('friction', 'NN'), ('between', 'IN'), ('the', 'DT'), ('U', 'NNP'), ('.', '.'), ('S', 'NNP'), ('.', '.'), ('And', 'CC'), ('Japan', 'NNP'), ('has', 'VBZ'), ('raised', 'VBN'), ('fears', 'NNS'), ('among', 'IN'), ('many', 'JJ'), ('of', 'IN'), ('Asia', 'NNP'), (\"'\", 'POS'), ('s', 'NN'), ('exporting', 'VBG'), ('nations', 'NNS'), ('that', 'IN'), ('the', 'DT'), ('row', 'NN'), ('could', 'MD'), ('inflict', 'VB'), ('far', 'RB'), ('-', ':'), ('reaching', 'VBG'), ('economic', 'JJ'), ('damage', 'NN'), (',', ','), ('businessmen', 'NNS'), ('and', 'CC'), ('officials', 'NNS'), ('said', 'VBD'), ('.', '.'), ('They', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "#tagging\n",
    "tagged = nltk.pos_tag(reuters1)\n",
    "print(tagged[:50])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
